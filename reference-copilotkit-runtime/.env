# Copy this file to `.env` for local development.

# Select one of: openai, anthropic, gemini, ollama
#LLM_PROVIDER=ollama

# Provider-specific configuration
# For ollama (local)
#OLLAMA_BASE_URL=http://localhost:11434

# For API providers (uncomment the one you use)
# OPENAI_API_KEY=sk-...
# ANTHROPIC_API_KEY=...
GEMINI_API_KEY=AIzaSyDaGF1a1MHyubHiULBKdQ3bTwDytP5-BUU #aipopstudio
GOOGLE_API_KEY=AIzaSyDaGF1a1MHyubHiULBKdQ3bTwDytP5-BUU #aipopstudio

# Model name for the selected provider
#MODEL_NAME=llama3.1

# Server bind settings
SERVER_HOST=127.0.0.1
SERVER_PORT=8000
